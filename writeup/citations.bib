@article{brief-summarization-survey,
    title   = {Text Summarization Techniques: A Brief Survey},
    volume  = {8},
    issn    = {21565570, 2158107X},
    url     = {http://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=ijacsa&SerialNo=52},
    doi     = {10.14569/IJACSA.2017.081052},
    number  = {10},
    journal = {International Journal of Advanced Computer Science and Applications},
    author  = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and D., Elizabeth and B., Juan and Kochut, Krys},
    year    = {2017}
}
@inproceedings{cidr-clustering,
    title     = {A Description of the Cidr System as Used for tdt-2},
    author    = {Dragomir Radev, VH and McKeowen, KR},
    booktitle = {Proc. DARPA Broadcast News Workshop, Herndon},
    year      = {1999}
}
@inproceedings{dataset-cnn,
    title     = {Teaching machines to read and comprehend},
    author    = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
    booktitle = {Advances in neural information processing systems},
    pages     = {1693--1701},
    year      = {2015}
}
@article{dataset-multinews,
    title         = {Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
    author        = {Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},
    year          = {2019},
    eprint        = {1906.01749},
    archiveprefix = {arXiv},
    primaryclass  = {cs.CL}
}

@book{information-retrieval,
    title     = {Introduction to information retrieval},
    author    = {Manning, Christopher D and Sch{\"u}tze, Hinrich and Raghavan, Prabhakar},
    year      = {2008},
    publisher = {Cambridge university press}
}
@inproceedings{lexpagerank-prestige,
    title     = {{L}ex{P}age{R}ank: Prestige in Multi-Document Text Summarization},
    author    = {Erkan, G{\"u}ne{\c{s}}  and
      Radev, Dragomir R.},
    booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
    month     = jul,
    year      = {2004},
    address   = {Barcelona, Spain},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/W04-3247},
    pages     = {365--371}
}
@article{lexrank-automatic-abstracting,
    title   = {Automatic Abstracting System Based on Improved LexRank Algorithm [J]},
    author  = {Zhou-jun, JI Wen-qian LI and Xiao-ming, CHAO Wen-han CHEN},
    journal = {Computer Science},
    volume  = {5},
    year    = {2010}
}
@inproceedings{lexrank-metaheuristic,
    place        = {Cham},
    series       = {Lecture Notes in Computer Science},
    title        = {Automatic Generation of Multi-document Summaries Based on the Global-Best Harmony Search Metaheuristic and the LexRank Graph-Based Algorithm},
    isbn         = {9783030028404},
    doi          = {10.1007/978-3-030-02840-4_7},
    abstractnote = {Recently, metaheuristic based algorithms have shown good results in generating automatic multi-document summaries. This paper proposes two algorithms that hybridize the metaheuristic of Global Best Harmony Search and the LexRank Graph based algorithm, called LexGbhs and GbhsLex. The objective function to be optimized is composed of the features of coverage and diversity. Coverage measures the similarity between each sentence of the candidate summary and the centroid of the sentences of the collection of documents, while diversity measures how different the sentences that make up a candidate summary are. The two proposed hybrid algorithms were compared with state of the art algorithms using ROUGE-1, ROUGE-2 and ROUGE-SU4 measurements for the DUC2005 and DUC2006 data sets. After a unified classification was carried out, the LexGbhs algorithm proposed ranked third, showing that the hybridization of metaheuristics with graphs in the generation of extractive summaries of multiple documents is a promising line of research.},
    booktitle    = {Advances in Computational Intelligence},
    publisher    = {Springer International Publishing},
    author       = {Cuéllar, César and Mendoza, Martha and Cobos, Carlos},
    editor       = {Castro, Félix and Miranda-Jiménez, Sabino and González-Mendoza, Miguel},
    year         = {2018},
    pages        = {82–94},
    collection   = {Lecture Notes in Computer Science}
}
@inproceedings{lexrank-opinionated,
    title        = {Research on Extension LexRank in Summarization for Opinionated Texts},
    issn         = {2379-5352},
    doi          = {10.1109/PDCAT.2012.117},
    abstractnote = {Along with the boom of information, it is an important task to summarize the valuable information of opinionated texts. Firstly, we model the opinionated text by TAM and get the topic and aspect attribute of every sentence. Secondly, we successively used the basic LexRank, Comparative LexRank, Topic-sensitive tf*idf LexRank and Topic-sensitive tf*idf & Comparative LexRank to generate summary. Experimental results show that the best summary on precision can be gotten by the topic-sensitive tf*idf LexRank and the best result on recall and F-measure can be gotten by the topic-sensitive tf*idf & comparative LexRank.},
    booktitle    = {2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies},
    author       = {Liang, X. and Qu, Y. and Ma, G.},
    year         = {2012},
    month        = {Dec},
    pages        = {517–522}
}
@article{lexrank-summarization,
    title        = {LexRank: Graph-based Lexical Centrality as Salience in Text Summarization},
    volume       = {22},
    issn         = {1076-9757},
    doi          = {10.1613/jair.1523},
    abstractnote = {We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.},
    journal      = {Journal of Artificial Intelligence Research},
    author       = {Erkan, G. and Radev, D. R.},
    year         = {2004},
    month        = {Dec},
    pages        = {457–479}
}
@misc{lexrank-tweet,
    title        = {Modified LexRank for Tweet Summarization},
    url          = {www.igi-global.com/article/modified-lexrank-for-tweet-summarization/163105},
    doi          = {10.4018/IJRSDA.2016100106},
    abstractnote = {Summary generation is an important process in those conditions where the user needs to obtain the key features of the document without having to go through the whole document itself. The summarization process is of basically two types: 1) Single document Summarization and, 2) Multiple Document Summa...},
    journal      = {International Journal of Rough Sets and Data Analysis (IJRSDA)},
    author       = {Samuel, Avinash and Sharma, Dilip Kumar},
    year         = {2016},
    month        = {Oct}
}
@article{lsa-elsa,
    author     = {Cagliero, Luca and Garza, Paolo and Baralis, Elena},
    title      = {ELSA: A Multilingual Document Summarization Algorithm Based on Frequent Itemsets and Latent Semantic Analysis},
    year       = {2019},
    issue_date = {March 2019},
    publisher  = {Association for Computing Machinery},
    address    = {New York, NY, USA},
    volume     = {37},
    number     = {2},
    issn       = {1046-8188},
    url        = {https://doi.org/10.1145/3298987},
    doi        = {10.1145/3298987},
    abstract   = {Sentence-based summarization aims at extracting concise summaries of collections of textual documents. Summaries consist of a worthwhile subset of document sentences. The most effective multilingual strategies rely on Latent Semantic Analysis (LSA) and on frequent itemset mining, respectively. LSA-based summarizers pick the document sentences that cover the most important concepts. Concepts are modeled as combinations of single-document terms and are derived from a term-by-sentence matrix by exploiting Singular Value Decomposition (SVD). Itemset-based summarizers pick the sentences that contain the largest number of frequent itemsets, which represent combinations of frequently co-occurring terms. The main drawbacks of existing approaches are (i)&nbsp;the inability of LSA to consider the correlation between combinations of multiple-document terms and the underlying concepts, (ii)&nbsp;the inherent redundancy of frequent itemsets because similar itemsets may be related to the same concept, and (iii)&nbsp;the inability of itemset-based summarizers to correlate itemsets with the underlying document concepts. To overcome the issues of both of the abovementioned algorithms, we propose a new summarization approach that exploits frequent itemsets to describe all of the latent concepts covered by the documents under analysis and LSA to reduce the potentially redundant set of itemsets to a compact set of uncorrelated concepts. The summarizer selects the sentences that cover the latent concepts with minimal redundancy. We tested the summarization algorithm on both multilingual and English-language benchmark document collections. The proposed approach performed significantly better than both itemset- and LSA-based summarizers, and better than most of the other state-of-the-art approaches.},
    journal    = {ACM Trans. Inf. Syst.},
    month      = jan,
    articleno  = {21},
    numpages   = {33},
    keywords   = {frequent weighted itemset mining, Multilingual summarization, text mining}
}
@inproceedings{lsa-implementation-naive,
    author    = {Gong, Yihong and Liu, Xin},
    title     = {Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis},
    year      = {2001},
    isbn      = {1581133316},
    publisher = {Association for Computing Machinery},
    address   = {New York, NY, USA},
    url       = {https://doi.org/10.1145/383952.383955},
    doi       = {10.1145/383952.383955},
    abstract  = {In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents. The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations. Both methods strive to select sentences that are highly ranked and different from each other. This is an attempt to create a summary with a wider coverage of the document's main content and less redundancy. Performance evaluations on the two summarization methods are conducted by comparing their summarization outputs with the manual summaries generated by three independent human evaluators. The evaluations also study the influence of different VSM weighting schemes on the text summarization performances. Finally, the causes of the large disparities in the evaluators' manual summarization results are investigated, and discussions on human text summarization patterns are presented.},
    booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages     = {19–25},
    numpages  = {7},
    keywords  = {generic text summarization, relevance measure, semantic analysis},
    location  = {New Orleans, Louisiana, USA},
    series    = {SIGIR '01}
}
@article{lsa-implementation-trm,
    title    = {Text summarization using a trainable summarizer and latent semantic analysis},
    journal  = {Information Processing & Management},
    volume   = {41},
    number   = {1},
    pages    = {75 - 95},
    year     = {2005},
    note     = {An Asian Digital Libraries Perspective},
    issn     = {0306-4573},
    doi      = {https://doi.org/10.1016/j.ipm.2004.04.003},
    url      = {http://www.sciencedirect.com/science/article/pii/S0306457304000329},
    author   = {Jen-Yuan Yeh and Hao-Ren Ke and Wei-Pang Yang and I-Heng Meng},
    keywords = {Text summarization, Corpus-based approach, Latent semantic analysis, Text relationship map},
    abstract = {This paper proposes two approaches to address text summarization: modified corpus-based approach (MCBA) and LSA-based T.R.M. approach (LSA+T.R.M.). The first is a trainable summarizer, which takes into account several features, including position, positive keyword, negative keyword, centrality, and the resemblance to the title, to generate summaries. Two new ideas are exploited: (1) sentence positions are ranked to emphasize the significances of different sentence positions, and (2) the score function is trained by the genetic algorithm (GA) to obtain a suitable combination of feature weights. The second uses latent semantic analysis (LSA) to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. We evaluate LSA+T.R.M. both with single documents and at the corpus level to investigate the competence of LSA in text summarization. The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles. When the compression rate was 30%, an average f-measure of 49% for MCBA, 52% for MCBA+GA, 44% and 40% for LSA+T.R.M. in single-document and corpus level were achieved respectively.}
}
@article{lsa-implementation-trm-original,
    title    = {Automatic text structuring and summarization},
    journal  = {Information Processing & Management},
    volume   = {33},
    number   = {2},
    pages    = {193 - 207},
    year     = {1997},
    note     = {Methods and Tools for the Automatic Construction of Hypertext},
    issn     = {0306-4573},
    doi      = {https://doi.org/10.1016/S0306-4573(96)00062-3},
    url      = {http://www.sciencedirect.com/science/article/pii/S0306457396000623},
    author   = {Gerard Salton and Amit Singhal and Mandar Mitra and Chris Buckley},
    abstract = {In recent years, information retrieval techniques have been used for automatic generation of semantic hypertext links. This study applies the ideas from the automatic link generation research to attack another important problem in text processing—automatic text summarization. An automatic “general purpose” text summarization tool would be of immense utility in this age of information overload. Using the techniques used (by most automatic hypertext link generation algorithms) for inter-document link generation, we generate intra-document links between passages of a document. Based on the intra-document linkage pattern of a text, we characterize the structure of the text. We apply the knowledge of text structure to do automatic text summarization by passage extraction. We evaluate a set of fifty summaries generated using our techniques by comparing them to paragraph extracts constructed by humans. The automatic summarization methods perform well, especially in view of the fact that the summaries generated by two humans for the same article are surprisingly dissimilar.}
}
@inproceedings{lsa-implementation-weighting,
    author = {Steinberger, Josef and Jezek, Karel},
    year   = {2004},
    month  = {01},
    pages  = {},
    title  = {Using Latent Semantic Analysis in Text Summarization and Summary Evaluation}
}
@article{lsa-overview,
    author   = {Makbule Gulcin Ozsoy and Ferda Nur Alpaslan and Ilyas Cicekli},
    title    = {Text summarization using Latent Semantic Analysis},
    journal  = {Journal of Information Science},
    volume   = {37},
    number   = {4},
    pages    = {405-417},
    year     = {2011},
    doi      = {10.1177/0165551511408848},
    url      = {
        https://doi.org/10.1177/0165551511408848

},
    eprint   = {
        https://doi.org/10.1177/0165551511408848

},
    abstract = { Text summarization solves the problem of presenting the information needed by a user in a compact form. There are different approaches to creating well-formed summaries. One of the newest methods is the Latent Semantic Analysis (LSA). In this paper, different LSA-based summarization algorithms are explained, two of which are proposed by the authors of this paper. The algorithms are evaluated on Turkish and English documents, and their performances are compared using their ROUGE scores. One of our algorithms produces the best scores and both algorithms perform equally well on Turkish and English document sets. }
}
@techreport{pagerank,
    number      = {1999-66},
    month       = {November},
    author      = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
    note        = {Previous number = SIDL-WP-1999-0120},
    title       = {The PageRank Citation Ranking: Bringing Order to the Web.},
    type        = {Technical Report},
    publisher   = {Stanford InfoLab},
    year        = {1999},
    institution = {Stanford InfoLab},
    url         = {http://ilpubs.stanford.edu:8090/422/},
    abstract    = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}
@article{sumbasic,
    title     = {The impact of frequency on summarization},
    author    = {Nenkova, Ani and Vanderwende, Lucy},
    journal   = {Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005},
    volume    = {101},
    year      = {2005},
    publisher = {Citeseer}
}
@inproceedings{sumbasic-extended,
    title     = {Multi-Document Summarization by Maximizing Informative Content-Words.},
    author    = {Yih, Wen-tau and Goodman, Joshua and Vanderwende, Lucy and Suzuki, Hisami},
    booktitle = {IJCAI},
    volume    = {7},
    pages     = {1776--1782},
    year      = {2007}
}
@inproceedings{text-summarization-techniques,
    title     = {A Survey of Text Summarization Techniques},
    author    = {A. Nenkova and K. McKeown},
    booktitle = {Mining Text Data},
    year      = {2012}
}
